# examples/demo_tokenize.py

from neetechs_tokenizer import tokenizer

text = "Ø§Ø³ØªÙƒØªØ¨ Ø§Ù„Ù…Ø¹Ù„Ù… Ø§Ù„Ø·Ø§Ù„Ø¨ Ø§Ù„Ø¯Ø±Ø³"

tokens = tokenizer.tokenize(text)
print("ğŸ”¹ Tokens:", tokens)

analysis = tokenizer.analyze_tokens(tokens)
print("\nğŸ” Analysis:")
for item in analysis:
    print(item)
